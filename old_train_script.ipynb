{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8dac69b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'torch.version' from '/home/sp4013/anaconda3/lib/python3.9/site-packages/torch/version.py'>\n",
      "cuda:0\n",
      "\n",
      "-----Getting Train/Test Data-----\n",
      "\n",
      "ENG (In-set)\n",
      "GER (In-set)\n",
      "ICE (In-set)\n",
      "FRE (In-set)\n",
      "SPA (In-set)\n",
      "ARA (In-set)\n",
      "RUS (In-set)\n",
      "BEN (In-set)\n",
      "KAS (In-set)\n",
      "GRE (In-set)\n",
      "CAT (In-set)\n",
      "KOR (In-set)\n",
      "TUR (In-set)\n",
      "TAM (In-set)\n",
      "TEL (In-set)\n",
      "CHI (In-set)\n",
      "TIB (In-set)\n",
      "JAV (In-set)\n",
      "EWE (In-set)\n",
      "HAU (In-set)\n",
      "LIN (In-set)\n",
      "YOR (In-set)\n",
      "HUN (In-set)\n",
      "HAW (In-set)\n",
      "MAO (In-set)\n",
      "ITA (In-set)\n",
      "URD (In-set)\n",
      "SWE (In-set)\n",
      "PUS (In-set)\n",
      "GEO (In-set)\n",
      "HIN (In-set)\n",
      "THA (In-set)\n",
      "\n",
      "Num chunks in training: 25176\n",
      "Num chunks in testing: 6308\n",
      "\n",
      "-----Finished Data Splitting-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import kaldiio\n",
    "import torch\n",
    "\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "print(torch.version)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "in_set = ['ENG', 'GER', 'ICE', 'FRE', 'SPA', 'ARA', 'RUS', 'BEN', 'KAS', 'GRE', 'CAT', 'KOR', 'TUR', 'TAM', 'TEL', 'CHI', 'TIB', 'JAV', 'EWE', 'HAU', 'LIN', 'YOR', 'HUN', 'HAW', 'MAO', 'ITA', 'URD', 'SWE', 'PUS', 'GEO', 'HIN', 'THA']\n",
    "out_of_set = ['DUT', 'HEB', 'UKR', 'BUL', 'PER', 'ALB', 'UIG', 'MAL', 'BUR', 'IBA', 'ASA', 'AKU', 'ARM', 'HRV', 'FIN', 'JPN', 'NOR', 'NEP', 'RUM']\n",
    "\n",
    "langs = in_set + out_of_set\n",
    "\n",
    "num_in_set = 32\n",
    "\n",
    "in_set = langs[:num_in_set]\n",
    "out_of_set = langs[num_in_set:]\n",
    "\n",
    "root_dir = \"/home/sp4013/kaldi-trunk/egs/lid/s1/db/cu-multilang-dataset/\"\n",
    "\n",
    "\n",
    "assert(len(in_set) + len(out_of_set) == 51)\n",
    "assert(len(set(in_set).intersection(set(out_of_set))) == 0)\n",
    "for lang in os.listdir(root_dir):\n",
    "    assert(lang in in_set or lang in out_of_set)\n",
    "\n",
    "print(\"\\n-----Getting Train/Test Data-----\\n\")\n",
    "\n",
    "train, test = [], []\n",
    "max_sample_length = 500\n",
    "num_chunks_per_file = 120\n",
    "num_files = 30\n",
    "\n",
    "for i,lang in enumerate(in_set, 0):\n",
    "    print(lang, \"(In-set)\" if lang in in_set else \"(Out-of-set)\")\n",
    "    \n",
    "    chunks = []\n",
    "    for f_idx in range(1, num_files):\n",
    "        if lang == \"ENG\"\n",
    "        \n",
    "        filepath = root_dir + lang + '/data/raw_mfcc_pitch_' + lang + '.' + str(f_idx) + '.ark'\n",
    "        file_chunks = []\n",
    "        \n",
    "        for key, numpy_array in kaldiio.load_ark(filepath):\n",
    "            curr_len = len(numpy_array)\n",
    "            \n",
    "#             chunks +=  np.split(numpy_array, np.arange(max_sample_length, curr_len, max_sample_length))\n",
    "            if curr_len >= max_sample_length:\n",
    "                file_chunks +=  np.split(numpy_array, np.arange(max_sample_length, curr_len, max_sample_length))[:-1]\n",
    "            else:\n",
    "                padded_chunk = np.pad(numpy_array, ((max_sample_length - curr_len, 0), (0, 0)), \"constant\")\n",
    "                file_chunks += [padded_chunk]\n",
    "                \n",
    "            if len(file_chunks) >= num_chunks_per_file:\n",
    "                chunks += file_chunks\n",
    "                break\n",
    "    \n",
    "    random.shuffle(chunks)\n",
    "\n",
    "    # Put chunks in train or test depending on in_set or out_of_set\n",
    "    switch_point = 0.80 if lang in in_set else 0.0\n",
    "    for j in range(len(chunks)):\n",
    "        chunk = chunks[j]\n",
    "        inputs = torch.from_numpy(np.expand_dims(chunk, axis=0))\n",
    "        inputs.to(device)\n",
    "        labels = torch.from_numpy(np.array([i if lang in in_set else -1]))\n",
    "        labels.to(device)\n",
    "\n",
    "        if j+1 <= switch_point * len(chunks):\n",
    "            train.append((inputs,labels))\n",
    "        else:\n",
    "            test.append((inputs,labels))\n",
    "\n",
    "print()\n",
    "print(\"Num chunks in training:\", len(train))\n",
    "print(\"Num chunks in testing:\", len(test))\n",
    "\n",
    "print(\"\\n-----Finished Data Splitting-----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c49ba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "            TDNN-1       [1, 254, 256]          21,248          21,248\n",
      "            TDNN-2       [1, 250, 256]         197,376         197,376\n",
      "            TDNN-3       [1, 244, 256]         197,376         197,376\n",
      "            TDNN-4       [1, 244, 256]          66,304          66,304\n",
      "            TDNN-5       [1, 244, 256]          66,304          66,304\n",
      "            TDNN-6        [1, 244, 32]           8,288           8,288\n",
      "=======================================================================\n",
      "Total params: 556,896\n",
      "Trainable params: 556,896\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from tdnn import TDNN\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = TDNN(input_dim=in_size, output_dim=256, context_size=5)\n",
    "        self.layer2 = TDNN(input_dim=256, output_dim=256, context_size=3, dilation=2)\n",
    "        self.layer3 = TDNN(input_dim=256, output_dim=256, context_size=3, dilation=3)\n",
    "        self.layer4 = TDNN(input_dim=256, output_dim=256, context_size=1)\n",
    "        self.layer5 = TDNN(input_dim=256, output_dim=256, context_size=1)\n",
    "        self.final_layer = TDNN(input_dim=256, output_dim=num_classes, context_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        forward_pass = nn.Sequential(\n",
    "            self.layer1,\n",
    "            nn.ReLU(),\n",
    "            self.layer2,\n",
    "            nn.ReLU(),\n",
    "            self.layer3,\n",
    "            nn.ReLU(),\n",
    "            self.layer4,\n",
    "            nn.ReLU(),\n",
    "            self.layer5,\n",
    "            nn.ReLU(),\n",
    "            self.final_layer,\n",
    "            nn.Softmax(dim=2))\n",
    "        \n",
    "        return forward_pass(x)\n",
    "    \n",
    "print(summary(Net(16, len(in_set)), torch.zeros((1, 258, 16)), show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b549f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, batch_size=64):\n",
    "    len_data = len(data)\n",
    "    num_batches = (len_data // batch_size) + 1\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len_data - 1)\n",
    "\n",
    "        yield data[start_idx: end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d36472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_model(model, loader, optimizer, criterion, device=None):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in loader:\n",
    "        num_batches += 1\n",
    "        len_batch = len(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0.0\n",
    "        acc = 0.0\n",
    "        for x, y in batch:\n",
    "          # Send to GPU if available\n",
    "          if device is not None:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "          y_pred = model(x)\n",
    "          y_pred = torch.mean(y_pred, 1)\n",
    "          loss += criterion(y_pred, y)\n",
    "          acc += calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        loss /= len_batch\n",
    "        acc /= len_batch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += acc.item()\n",
    "\n",
    "    return train_loss / num_batches, train_acc / num_batches\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device=None):\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "#     model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            num_batches += 1\n",
    "            len_batch = len(batch)\n",
    "            loss = 0.0\n",
    "            acc = 0.0\n",
    "\n",
    "            for x, y in batch:\n",
    "              # Send to GPU if available\n",
    "              if device is not None:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "              y_pred = model(x)\n",
    "              y_pred = torch.mean(y_pred, 1)\n",
    "              loss += criterion(y_pred, y)\n",
    "              acc += calculate_accuracy(y_pred, y)\n",
    "\n",
    "            loss /= len_batch\n",
    "            acc /= len_batch\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_acc += acc.item()\n",
    "\n",
    "    return val_loss / num_batches, val_acc / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb77cd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training\n",
      "Epoch: 0, Train Loss: 3.447267251571423, Train Accuracy: 0.19780401681280377, Val Loss: 3.430316033989492, Val Accuracy: 0.37904942280264814\n",
      "Epoch: 1, Train Loss: 3.4077004091388683, Train Accuracy: 0.41236757891734843, Val Loss: 3.3805393474270597, Val Accuracy: 0.46916937229759764\n",
      "Epoch: 2, Train Loss: 3.3536054240870596, Train Accuracy: 0.49575493828899364, Val Loss: 3.32689153064381, Val Accuracy: 0.5159406565656566\n",
      "Epoch: 3, Train Loss: 3.295292195934935, Train Accuracy: 0.5752938093267722, Val Loss: 3.2733273891487507, Val Accuracy: 0.5869588745061798\n",
      "Epoch: 4, Train Loss: 3.2416426009938197, Train Accuracy: 0.6204687155745356, Val Loss: 3.2275097225651597, Val Accuracy: 0.6234442641337713\n",
      "Epoch: 5, Train Loss: 3.196251704003, Train Accuracy: 0.6502203570707196, Val Loss: 3.1809383474215114, Val Accuracy: 0.6543650795715024\n",
      "Epoch: 6, Train Loss: 3.1590693087747255, Train Accuracy: 0.6701507669717527, Val Loss: 3.1551020025002834, Val Accuracy: 0.6571744228854324\n",
      "Epoch: 7, Train Loss: 3.1304779034580674, Train Accuracy: 0.6846653608803822, Val Loss: 3.1248195701175265, Val Accuracy: 0.6849567101459311\n",
      "Epoch: 8, Train Loss: 3.1024534944350344, Train Accuracy: 0.7045026622750432, Val Loss: 3.099141908414436, Val Accuracy: 0.6971139974064298\n",
      "Epoch: 9, Train Loss: 3.0812542940759418, Train Accuracy: 0.7164395277572767, Val Loss: 3.0829562008982956, Val Accuracy: 0.6960542930497063\n",
      "Epoch: 10, Train Loss: 3.0615951766822542, Train Accuracy: 0.7277556692585727, Val Loss: 3.0615394548936323, Val Accuracy: 0.7239402960045169\n",
      "Epoch: 11, Train Loss: 3.0389531524048237, Train Accuracy: 0.7498758553248371, Val Loss: 3.0484609194476193, Val Accuracy: 0.7291756856321084\n",
      "Epoch: 12, Train Loss: 3.024647344792555, Train Accuracy: 0.760966122301702, Val Loss: 3.0432831301833643, Val Accuracy: 0.7351776695612705\n",
      "Epoch: 13, Train Loss: 3.004291654238241, Train Accuracy: 0.7808965322027351, Val Loss: 3.017551898956299, Val Accuracy: 0.7521645023365213\n",
      "Epoch: 14, Train Loss: 2.9891273134250933, Train Accuracy: 0.7936834446367274, Val Loss: 3.0009313472593675, Val Accuracy: 0.7710001805816034\n",
      "Best Val Loss: 3.0009313472593675 at Epoch: 14\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TDNN, loss, and optimizer\n",
    "# print(len(in_set))\n",
    "net = Net(16, len(in_set))\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss() # a common loss function for multi-class classification problems\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) # a common optimizer for multi-class classification problems\n",
    "\n",
    "\n",
    "# Train the TDNN network\n",
    "do_training = True # whether to train or to load a saved model\n",
    "SAVE_PATH = 'saved_models/tdnn.pth'\n",
    "LOAD_PATH = 'saved_models/tdnn.pth'\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "if do_training:\n",
    "    print('Started Training')\n",
    "\n",
    "    for epoch in range(15):  # number of epochs\n",
    "        random.shuffle(train) # shuffle data every epoch\n",
    "        train_loss, train_accuracy = train_model(net, create_batches(train, 64), optimizer, criterion, device)\n",
    "        val_loss, val_accuracy = evaluate_model(net, create_batches(test, 64), criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "          best_loss = val_loss\n",
    "          best_epoch = epoch\n",
    "          torch.save(net.state_dict(), SAVE_PATH) # Save the model\n",
    "\n",
    "\n",
    "        print(\"Epoch: \" + str(epoch) + \", Train Loss: \" + str(train_loss) + \", Train Accuracy: \" + str(train_accuracy) + \\\n",
    "            \", Val Loss: \" + str(val_loss) + \", Val Accuracy: \" + str(val_accuracy))\n",
    "\n",
    "\n",
    "    print(\"Best Val Loss: \" + str(best_loss) + \" at Epoch: \" + str(best_epoch))\n",
    "                \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "070c0d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENG (In-set)\n",
      "GER (In-set)\n",
      "ICE (In-set)\n",
      "FRE (In-set)\n",
      "SPA (In-set)\n",
      "ARA (In-set)\n",
      "RUS (In-set)\n",
      "BEN (In-set)\n",
      "KAS (In-set)\n",
      "GRE (In-set)\n",
      "CAT (In-set)\n",
      "KOR (In-set)\n",
      "TUR (In-set)\n",
      "TAM (In-set)\n",
      "TEL (In-set)\n",
      "CHI (In-set)\n",
      "TIB (In-set)\n",
      "JAV (In-set)\n",
      "EWE (In-set)\n",
      "HAU (In-set)\n",
      "LIN (In-set)\n",
      "YOR (In-set)\n",
      "HUN (In-set)\n",
      "HAW (In-set)\n",
      "MAO (In-set)\n",
      "ITA (In-set)\n",
      "URD (In-set)\n",
      "SWE (In-set)\n",
      "PUS (In-set)\n",
      "GEO (In-set)\n",
      "HIN (In-set)\n",
      "THA (In-set)\n",
      "DUT (Out-of-set)\n",
      "HEB (Out-of-set)\n",
      "UKR (Out-of-set)\n",
      "BUL (Out-of-set)\n",
      "PER (Out-of-set)\n",
      "ALB (Out-of-set)\n",
      "UIG (Out-of-set)\n",
      "MAL (Out-of-set)\n",
      "BUR (Out-of-set)\n",
      "IBA (Out-of-set)\n",
      "ASA (Out-of-set)\n",
      "AKU (Out-of-set)\n",
      "ARM (Out-of-set)\n",
      "HRV (Out-of-set)\n",
      "FIN (Out-of-set)\n",
      "JPN (Out-of-set)\n",
      "NOR (Out-of-set)\n",
      "NEP (Out-of-set)\n",
      "RUM (Out-of-set)\n",
      "6133\n"
     ]
    }
   ],
   "source": [
    "# Add out of set to test set\n",
    "test = []\n",
    "for i,lang in enumerate(in_set + out_of_set, 0):\n",
    "    print(lang, \"(In-set)\" if lang in in_set else \"(Out-of-set)\")\n",
    "    filepath = root_dir + lang + '/data/raw_mfcc_pitch_' + lang + '.25.ark'\n",
    "\n",
    "    chunks = []\n",
    "    for key, numpy_array in kaldiio.load_ark(filepath):\n",
    "        chunks +=  np.split(numpy_array, np.arange(50000, len(numpy_array), 50000))\n",
    "    random.shuffle(chunks)\n",
    "\n",
    "    for j in range(len(chunks)):\n",
    "        chunk = chunks[j]\n",
    "        inputs = torch.from_numpy(np.expand_dims(chunk, axis=0))\n",
    "        inputs.to(device)\n",
    "        labels = torch.from_numpy(np.array([i if lang in in_set else -1]))\n",
    "        labels.to(device)\n",
    "\n",
    "        test.append((inputs,labels))\n",
    "        \n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48261328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0.4265449209196152\n",
      "0.3 0.45361160932659383\n",
      "0.4 0.48622207728680905\n",
      "0.5 0.5144301320723952\n",
      "0.6 0.4920919615196478\n",
      "0.7 0.4053481167454753\n",
      "0.8 0.28697211804989403\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(LOAD_PATH))\n",
    "\n",
    "for thresh in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "    correct = 0\n",
    "    for x, y in test:\n",
    "      # Send to GPU if available\n",
    "      if device is not None:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "      y_pred = net(x)\n",
    "      y_pred = torch.mean(y_pred, 1)\n",
    "      conf = y_pred.amax(1, keepdim=True)\n",
    "\n",
    "      if conf > thresh:\n",
    "          top_pred = y_pred.argmax(1, keepdim=True)\n",
    "      else:\n",
    "          top_pred = -1\n",
    "\n",
    "      if top_pred == y:\n",
    "        correct += 1\n",
    "        \n",
    "    print(thresh, correct / len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5880d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
