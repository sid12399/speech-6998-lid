{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8dac69b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'torch.version' from '/home/sp4013/anaconda3/lib/python3.9/site-packages/torch/version.py'>\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import kaldiio\n",
    "import torch\n",
    "\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "print(torch.version)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "in_set = ['ENG', 'GER', 'ICE', 'FRE', 'SPA', 'ARA', 'RUS', 'BEN', 'KAS', 'GRE', 'CAT', 'KOR', 'TUR', 'TAM', 'TEL', 'CHI', 'TIB', 'JAV', 'EWE', 'HAU', 'LIN', 'YOR', 'HUN', 'HAW', 'MAO', 'ITA', 'URD', 'SWE', 'PUS', 'GEO', 'HIN', 'THA']\n",
    "out_of_set = ['DUT', 'HEB', 'UKR', 'BUL', 'PER', 'ALB', 'UIG', 'MAL', 'BUR', 'IBA', 'ASA', 'AKU', 'ARM', 'HRV', 'FIN', 'JPN', 'NOR', 'NEP', 'RUM']\n",
    "\n",
    "langs = in_set + out_of_set\n",
    "\n",
    "num_in_set = 32\n",
    "\n",
    "in_set = langs[:num_in_set]\n",
    "out_of_set = langs[num_in_set:]\n",
    "\n",
    "root_dir = \"/home/sp4013/kaldi-trunk/egs/lid/s1/db/cu-multilang-dataset/\"\n",
    "\n",
    "\n",
    "assert(len(in_set) + len(out_of_set) == 51)\n",
    "assert(len(set(in_set).intersection(set(out_of_set))) == 0)\n",
    "for lang in os.listdir(root_dir):\n",
    "    assert(lang in in_set or lang in out_of_set)\n",
    "\n",
    "# print(\"\\n-----Getting Train/Test Data-----\\n\")\n",
    "\n",
    "# train, test = [], []\n",
    "# max_sample_length = 300\n",
    "# num_chunks_per_file = 120\n",
    "# num_files = 30\n",
    "\n",
    "# for i,lang in enumerate(in_set + out_of_set, 0):\n",
    "#     print(lang, \"(In-set)\" if lang in in_set else \"(Out-of-set)\")\n",
    "    \n",
    "#     chunks = []\n",
    "#     for f_idx in range(1, num_files + 1):\n",
    "        \n",
    "#         filepath = root_dir + lang + '/data/raw_mfcc_pitch_' + lang + '.' + str(f_idx) + '.ark'\n",
    "#         file_chunks = []\n",
    "        \n",
    "#         for key, numpy_array in kaldiio.load_ark(filepath):\n",
    "#             curr_len = len(numpy_array)\n",
    "\n",
    "# #             chunks +=  np.split(numpy_array, np.arange(max_sample_length, curr_len, max_sample_length))\n",
    "#             if curr_len >= max_sample_length:\n",
    "#                 file_chunks +=  np.split(numpy_array, np.arange(max_sample_length, curr_len, max_sample_length))[:-1]\n",
    "# #             else:\n",
    "# #                 padded_chunk = np.pad(numpy_array, ((max_sample_length - curr_len, 0), (0, 0)), \"constant\")\n",
    "# #                 file_chunks += [padded_chunk]\n",
    "                \n",
    "# #             if len(file_chunks) >= num_chunks_per_file:\n",
    "#                 chunks += file_chunks\n",
    "# #                 break\n",
    "    \n",
    "#     random.shuffle(chunks)\n",
    "\n",
    "#     # Put chunks in train or test depending on in_set or out_of_set\n",
    "#     switch_point = 0.80 if lang in in_set else 0.0\n",
    "#     for j in range(len(chunks)):\n",
    "#         chunk = chunks[j]\n",
    "#         inputs = torch.from_numpy(np.expand_dims(chunk, axis=0))\n",
    "#         inputs.to(device)\n",
    "#         labels = torch.from_numpy(np.array([i if lang in in_set else -1]))\n",
    "#         labels.to(device)\n",
    "\n",
    "#         if j+1 <= switch_point * len(chunks):\n",
    "#             train.append((inputs,labels))\n",
    "#         else:\n",
    "#             test.append((inputs,labels))\n",
    "\n",
    "# print()\n",
    "# print(\"Num chunks in training:\", len(train))\n",
    "# print(\"Num chunks in testing:\", len(test))\n",
    "\n",
    "# print(\"\\n-----Finished Data Splitting-----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "221896d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENG (In-set)\n",
      "GER (In-set)\n",
      "ICE (In-set)\n",
      "FRE (In-set)\n",
      "SPA (In-set)\n",
      "ARA (In-set)\n",
      "RUS (In-set)\n",
      "BEN (In-set)\n",
      "KAS (In-set)\n",
      "GRE (In-set)\n",
      "CAT (In-set)\n",
      "KOR (In-set)\n",
      "TUR (In-set)\n",
      "TAM (In-set)\n",
      "TEL (In-set)\n",
      "CHI (In-set)\n",
      "TIB (In-set)\n",
      "JAV (In-set)\n",
      "EWE (In-set)\n",
      "HAU (In-set)\n",
      "LIN (In-set)\n",
      "YOR (In-set)\n",
      "HUN (In-set)\n",
      "HAW (In-set)\n",
      "MAO (In-set)\n",
      "ITA (In-set)\n",
      "URD (In-set)\n",
      "SWE (In-set)\n",
      "PUS (In-set)\n",
      "GEO (In-set)\n",
      "HIN (In-set)\n",
      "THA (In-set)\n",
      "DUT (Out-of-set)\n",
      "HEB (Out-of-set)\n",
      "UKR (Out-of-set)\n",
      "BUL (Out-of-set)\n",
      "PER (Out-of-set)\n",
      "ALB (Out-of-set)\n",
      "UIG (Out-of-set)\n",
      "MAL (Out-of-set)\n",
      "BUR (Out-of-set)\n",
      "IBA (Out-of-set)\n",
      "ASA (Out-of-set)\n",
      "AKU (Out-of-set)\n",
      "ARM (Out-of-set)\n",
      "HRV (Out-of-set)\n",
      "FIN (Out-of-set)\n",
      "JPN (Out-of-set)\n",
      "NOR (Out-of-set)\n",
      "NEP (Out-of-set)\n",
      "RUM (Out-of-set)\n",
      "combining all mfccs by label\n",
      "chunking and splitting into 3 groups\n",
      "\n",
      "train1: 246728\n",
      "train2: 86336\n",
      "test: 34586\n",
      "\ttest (in-set): 13001\n",
      "\ttest (oos): 21585\n",
      "\n",
      "labels in test: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, -1}\n",
      "torch.Size([1, 400, 16])\n",
      "[(-1, 21585), (0, 213), (1, 449), (2, 449), (3, 449), (4, 450), (5, 450), (6, 449), (7, 448), (8, 152), (9, 177), (10, 423), (11, 449), (12, 450), (13, 318), (14, 256), (15, 448), (16, 450), (17, 449), (18, 450), (19, 449), (20, 450), (21, 450), (22, 450), (23, 449), (24, 449), (25, 450), (26, 450), (27, 450), (28, 450), (29, 450), (30, 226), (31, 449)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "data = []\n",
    "for i,lang in enumerate(in_set + out_of_set, 0):\n",
    "    print(lang, \"(In-set)\" if lang in in_set else \"(Out-of-set)\")\n",
    "    \n",
    "    for f_idx in range(1, 30 + 1):\n",
    "        \n",
    "        filepath = root_dir + lang + '/data/raw_mfcc_pitch_' + lang + '.' + str(f_idx) + '.ark'\n",
    "\n",
    "        for key, numpy_array in kaldiio.load_ark(filepath):\n",
    "            inputs = torch.from_numpy(np.expand_dims(numpy_array, axis=0))\n",
    "            inputs.to(device)\n",
    "            labels = torch.from_numpy(np.array([i if lang in in_set else -1]))\n",
    "            labels.to(device)\n",
    "            data.append((inputs, labels))\n",
    "\n",
    "\n",
    "print(\"combining all mfccs by label\")\n",
    "random.shuffle(data)\n",
    "data_concatenated = dict()\n",
    "for iter,i in enumerate(data): \n",
    "    label = i[1].numpy()[0]\n",
    "    mfcc = np.squeeze(i[0].numpy(), axis=0)\n",
    "    if label in data_concatenated:\n",
    "        data_concatenated[label].append(mfcc)\n",
    "    else:\n",
    "        data_concatenated[label] = [mfcc]\n",
    "\n",
    "for i in data_concatenated:\n",
    "    data_concatenated[i] = np.vstack(data_concatenated[i])\n",
    "\n",
    "del data\n",
    "\n",
    "def chunkify_tensor(tensor, size=400):\n",
    "    return torch.split(tensor, size, dim=1)[:-1] # except last one bc that isn't the right size\n",
    "\n",
    "'''\n",
    "Train (tdnn):       95% of in-set\n",
    "Train (lda/plda):   80% of out-of-set\n",
    "Test:               5% of in-set + 20% out-of-set\n",
    "'''\n",
    "\n",
    "print(\"chunking and splitting into 3 groups\")\n",
    "train1, train2, test = [], [], []\n",
    "for i in data_concatenated:\n",
    "    label = torch.from_numpy(np.array([i]))\n",
    "    mfcc = torch.from_numpy(np.expand_dims(data_concatenated[i], axis=0))\n",
    "    chunks = chunkify_tensor(mfcc)\n",
    "    if i >= 0:\n",
    "        cutoff = int(len(chunks) * 0.95)\n",
    "        for chunk in chunks[:cutoff]:\n",
    "            train1.append((chunk.to(device), label.to(device)))\n",
    "        for chunk in chunks[cutoff:]:\n",
    "            test.append((chunk.to(device), label.to(device)))\n",
    "    else:\n",
    "        cutoff = int(len(chunks) * 0.8)\n",
    "        for chunk in chunks[:cutoff]:\n",
    "            train2.append((chunk.to(device), label.to(device)))\n",
    "        for chunk in chunks[cutoff:]:\n",
    "            test.append((chunk.to(device), label.to(device)))\n",
    "\n",
    "del data_concatenated\n",
    "\n",
    "print()\n",
    "print(\"train1:\", len(train1))\n",
    "print(\"train2:\", len(train2))\n",
    "print(\"test:\", len(test))\n",
    "print(\"\\ttest (in-set):\", len([i for i in test if i[1].to('cpu').numpy()[0] >= 0]))\n",
    "print(\"\\ttest (oos):\", len([i for i in test if i[1].to('cpu').numpy()[0] < 0]))\n",
    "print()\n",
    "print(\"labels in test:\", set([i[1].to('cpu').numpy()[0] for i in test]))\n",
    "print(test[0][0].size())\n",
    "asdfasdfasdf = defaultdict(int)\n",
    "for i in test:\n",
    "    asdfasdf = i[1].to('cpu').numpy()[0]\n",
    "    asdfasdfasdf[asdfasdf] += 1\n",
    "print(sorted(asdfasdfasdf.items()))\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e774cb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{19: 449, -1: 21585, 6: 449, 3: 449, 15: 448, 5: 450, 13: 318, 4: 450, 0: 213, 30: 226, 7: 448, 2: 449, 24: 449, 17: 449, 23: 449, 31: 449, 28: 450, 1: 449, 16: 450, 18: 450, 11: 449, 9: 177, 29: 450, 25: 450, 20: 450, 14: 256, 22: 450, 12: 450, 26: 450, 27: 450, 21: 450, 10: 423, 8: 152}\n"
     ]
    }
   ],
   "source": [
    "chunk_count = {}\n",
    "\n",
    "for x, y in test:\n",
    "    i = y.item()\n",
    "    if i not in chunk_count:\n",
    "        chunk_count[i] = 1\n",
    "    else:\n",
    "        chunk_count[i] += 1\n",
    "\n",
    "print(chunk_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c49ba73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "            TDNN-1       [1, 396, 256]          21,248          21,248\n",
      "            TDNN-2       [1, 392, 256]         197,376         197,376\n",
      "            TDNN-3       [1, 386, 256]         197,376         197,376\n",
      "            TDNN-4       [1, 386, 256]          66,304          66,304\n",
      "            TDNN-5       [1, 386, 256]          66,304          66,304\n",
      "            TDNN-6        [1, 386, 32]           8,288           8,288\n",
      "=======================================================================\n",
      "Total params: 556,896\n",
      "Trainable params: 556,896\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from tdnn import TDNN\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = TDNN(input_dim=in_size, output_dim=256, context_size=5)\n",
    "        self.layer2 = TDNN(input_dim=256, output_dim=256, context_size=3, dilation=2)\n",
    "        self.layer3 = TDNN(input_dim=256, output_dim=256, context_size=3, dilation=3)\n",
    "        self.layer4 = TDNN(input_dim=256, output_dim=256, context_size=1)\n",
    "        self.layer5 = TDNN(input_dim=256, output_dim=256, context_size=1)\n",
    "        self.final_layer = TDNN(input_dim=256, output_dim=num_classes, context_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        forward_pass = nn.Sequential(\n",
    "            self.layer1,\n",
    "            nn.ReLU(),\n",
    "            self.layer2,\n",
    "            nn.ReLU(),\n",
    "            self.layer3,\n",
    "            nn.ReLU(),\n",
    "            self.layer4,\n",
    "            nn.ReLU(),\n",
    "            self.layer5,\n",
    "            nn.ReLU(),\n",
    "            self.final_layer)\n",
    "        \n",
    "        return forward_pass(x)\n",
    "    \n",
    "print(summary(Net(16, len(in_set)), torch.zeros((1, 400, 16)), show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23f84117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34586\n",
      "0.0 0.3478575146012838\n",
      "0.4 0.7059214711154802\n",
      "0.5 0.7727404151969005\n",
      "0.6 0.8186260336552362\n",
      "0.7 0.8499681952235009\n",
      "0.8 0.8705834730815937\n",
      "0.9 0.8764818134505291\n"
     ]
    }
   ],
   "source": [
    "from pickle import Pickler, Unpickler\n",
    "\n",
    "infile = open(\"saved_models/tdnn-256-4s-25epochs.pickle\", \"rb\")\n",
    "new_net = Unpickler(infile).load()\n",
    "infile.close()\n",
    "\n",
    "# new_net = Net(16, 32)\n",
    "# new_net.load_state_dict(torch.load(\"saved_models/tdnn-256-4s.pth\"))\n",
    "# new_net.to(device)\n",
    "\n",
    "# out_len = len(test)\n",
    "# count = 0\n",
    "# for x, y in val_set:\n",
    "#     test.append((x.unsqueeze(0), y))\n",
    "#     count += 1\n",
    "#     if count == out_len:\n",
    "#         break\n",
    "    \n",
    "print(len(test))\n",
    "\n",
    "for thresh in [0.0, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]:\n",
    "    correct = 0\n",
    "    for x, y in test:\n",
    "      # Send to GPU if available\n",
    "      if device is not None:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "      y_pred = new_net(x)\n",
    "      y_pred = torch.mean(y_pred, 1)\n",
    "#       top_pred = y_pred.argmax(1, keepdim=True)\n",
    "      y_pred = F.softmax(y_pred, dim=1)\n",
    "#       print(y_pred)\n",
    "    \n",
    "      conf = y_pred.amax(1, keepdim=True).item()\n",
    "      if conf > thresh:\n",
    "          top_pred = y_pred.argmax(1, keepdim=True)\n",
    "      else:\n",
    "          top_pred = -1\n",
    "\n",
    "      if top_pred == y:\n",
    "        correct += 1\n",
    "    \n",
    "    print(thresh, correct / len(test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b549f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, batch_size=64):\n",
    "    len_data = len(data)\n",
    "    num_batches = (len_data // batch_size) + 1\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len_data - 1)\n",
    "\n",
    "        yield data[start_idx: end_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0d36472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_model(model, loader, optimizer, criterion, device=None):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in loader:\n",
    "        num_batches += 1\n",
    "        len_batch = len(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0.0\n",
    "        acc = 0.0\n",
    "        for x, y in batch:\n",
    "          # Send to GPU if available\n",
    "          if device is not None:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "          y_pred = model(x)\n",
    "          y_pred = torch.mean(y_pred, 1)\n",
    "          loss += criterion(y_pred, y)\n",
    "          acc += calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        loss /= len_batch\n",
    "        acc /= len_batch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += acc.item()\n",
    "\n",
    "    return train_loss / num_batches, train_acc / num_batches\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device=None):\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "#     model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            num_batches += 1\n",
    "            len_batch = len(batch)\n",
    "            loss = 0.0\n",
    "            acc = 0.0\n",
    "\n",
    "            for x, y in batch:\n",
    "              # Send to GPU if available\n",
    "              if device is not None:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "              y_pred = model(x)\n",
    "              y_pred = torch.mean(y_pred, 1)\n",
    "              loss += criterion(y_pred, y)\n",
    "              acc += calculate_accuracy(y_pred, y)\n",
    "\n",
    "            loss /= len_batch\n",
    "            acc /= len_batch\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_acc += acc.item()\n",
    "\n",
    "    return val_loss / num_batches, val_acc / num_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb77cd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training\n",
      "Epoch: 0, Train Loss: 3.447267251571423, Train Accuracy: 0.19780401681280377, Val Loss: 3.430316033989492, Val Accuracy: 0.37904942280264814\n",
      "Epoch: 1, Train Loss: 3.4077004091388683, Train Accuracy: 0.41236757891734843, Val Loss: 3.3805393474270597, Val Accuracy: 0.46916937229759764\n",
      "Epoch: 2, Train Loss: 3.3536054240870596, Train Accuracy: 0.49575493828899364, Val Loss: 3.32689153064381, Val Accuracy: 0.5159406565656566\n",
      "Epoch: 3, Train Loss: 3.295292195934935, Train Accuracy: 0.5752938093267722, Val Loss: 3.2733273891487507, Val Accuracy: 0.5869588745061798\n",
      "Epoch: 4, Train Loss: 3.2416426009938197, Train Accuracy: 0.6204687155745356, Val Loss: 3.2275097225651597, Val Accuracy: 0.6234442641337713\n",
      "Epoch: 5, Train Loss: 3.196251704003, Train Accuracy: 0.6502203570707196, Val Loss: 3.1809383474215114, Val Accuracy: 0.6543650795715024\n",
      "Epoch: 6, Train Loss: 3.1590693087747255, Train Accuracy: 0.6701507669717527, Val Loss: 3.1551020025002834, Val Accuracy: 0.6571744228854324\n",
      "Epoch: 7, Train Loss: 3.1304779034580674, Train Accuracy: 0.6846653608803822, Val Loss: 3.1248195701175265, Val Accuracy: 0.6849567101459311\n",
      "Epoch: 8, Train Loss: 3.1024534944350344, Train Accuracy: 0.7045026622750432, Val Loss: 3.099141908414436, Val Accuracy: 0.6971139974064298\n",
      "Epoch: 9, Train Loss: 3.0812542940759418, Train Accuracy: 0.7164395277572767, Val Loss: 3.0829562008982956, Val Accuracy: 0.6960542930497063\n",
      "Epoch: 10, Train Loss: 3.0615951766822542, Train Accuracy: 0.7277556692585727, Val Loss: 3.0615394548936323, Val Accuracy: 0.7239402960045169\n",
      "Epoch: 11, Train Loss: 3.0389531524048237, Train Accuracy: 0.7498758553248371, Val Loss: 3.0484609194476193, Val Accuracy: 0.7291756856321084\n",
      "Epoch: 12, Train Loss: 3.024647344792555, Train Accuracy: 0.760966122301702, Val Loss: 3.0432831301833643, Val Accuracy: 0.7351776695612705\n",
      "Epoch: 13, Train Loss: 3.004291654238241, Train Accuracy: 0.7808965322027351, Val Loss: 3.017551898956299, Val Accuracy: 0.7521645023365213\n",
      "Epoch: 14, Train Loss: 2.9891273134250933, Train Accuracy: 0.7936834446367274, Val Loss: 3.0009313472593675, Val Accuracy: 0.7710001805816034\n",
      "Best Val Loss: 3.0009313472593675 at Epoch: 14\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TDNN, loss, and optimizer\n",
    "# print(len(in_set))\n",
    "net = Net(16, len(in_set))\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss() # a common loss function for multi-class classification problems\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) # a common optimizer for multi-class classification problems\n",
    "\n",
    "\n",
    "# Train the TDNN network\n",
    "do_training = True # whether to train or to load a saved model\n",
    "SAVE_PATH = 'saved_models/tdnn.pth'\n",
    "LOAD_PATH = 'saved_models/tdnn.pth'\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "if do_training:\n",
    "    print('Started Training')\n",
    "\n",
    "    for epoch in range(15):  # number of epochs\n",
    "        random.shuffle(train) # shuffle data every epoch\n",
    "        train_loss, train_accuracy = train_model(net, create_batches(train, 64), optimizer, criterion, device)\n",
    "        val_loss, val_accuracy = evaluate_model(net, create_batches(test, 64), criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "          best_loss = val_loss\n",
    "          best_epoch = epoch\n",
    "          torch.save(net.state_dict(), SAVE_PATH) # Save the model\n",
    "\n",
    "\n",
    "        print(\"Epoch: \" + str(epoch) + \", Train Loss: \" + str(train_loss) + \", Train Accuracy: \" + str(train_accuracy) + \\\n",
    "            \", Val Loss: \" + str(val_loss) + \", Val Accuracy: \" + str(val_accuracy))\n",
    "\n",
    "\n",
    "    print(\"Best Val Loss: \" + str(best_loss) + \" at Epoch: \" + str(best_epoch))\n",
    "                \n",
    "    print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "070c0d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENG (In-set)\n",
      "GER (In-set)\n",
      "ICE (In-set)\n",
      "FRE (In-set)\n",
      "SPA (In-set)\n",
      "ARA (In-set)\n",
      "RUS (In-set)\n",
      "BEN (In-set)\n",
      "KAS (In-set)\n",
      "GRE (In-set)\n",
      "CAT (In-set)\n",
      "KOR (In-set)\n",
      "TUR (In-set)\n",
      "TAM (In-set)\n",
      "TEL (In-set)\n",
      "CHI (In-set)\n",
      "TIB (In-set)\n",
      "JAV (In-set)\n",
      "EWE (In-set)\n",
      "HAU (In-set)\n",
      "LIN (In-set)\n",
      "YOR (In-set)\n",
      "HUN (In-set)\n",
      "HAW (In-set)\n",
      "MAO (In-set)\n",
      "ITA (In-set)\n",
      "URD (In-set)\n",
      "SWE (In-set)\n",
      "PUS (In-set)\n",
      "GEO (In-set)\n",
      "HIN (In-set)\n",
      "THA (In-set)\n",
      "DUT (Out-of-set)\n",
      "HEB (Out-of-set)\n",
      "UKR (Out-of-set)\n",
      "BUL (Out-of-set)\n",
      "PER (Out-of-set)\n",
      "ALB (Out-of-set)\n",
      "UIG (Out-of-set)\n",
      "MAL (Out-of-set)\n",
      "BUR (Out-of-set)\n",
      "IBA (Out-of-set)\n",
      "ASA (Out-of-set)\n",
      "AKU (Out-of-set)\n",
      "ARM (Out-of-set)\n",
      "HRV (Out-of-set)\n",
      "FIN (Out-of-set)\n",
      "JPN (Out-of-set)\n",
      "NOR (Out-of-set)\n",
      "NEP (Out-of-set)\n",
      "RUM (Out-of-set)\n",
      "6133\n"
     ]
    }
   ],
   "source": [
    "# Add out of set to test set\n",
    "test = []\n",
    "for i,lang in enumerate(in_set + out_of_set, 0):\n",
    "    print(lang, \"(In-set)\" if lang in in_set else \"(Out-of-set)\")\n",
    "    filepath = root_dir + lang + '/data/raw_mfcc_pitch_' + lang + '.25.ark'\n",
    "\n",
    "    chunks = []\n",
    "    for key, numpy_array in kaldiio.load_ark(filepath):\n",
    "        chunks +=  np.split(numpy_array, np.arange(50000, len(numpy_array), 50000))\n",
    "    random.shuffle(chunks)\n",
    "\n",
    "    for j in range(len(chunks)):\n",
    "        chunk = chunks[j]\n",
    "        inputs = torch.from_numpy(np.expand_dims(chunk, axis=0))\n",
    "        inputs.to(device)\n",
    "        labels = torch.from_numpy(np.array([i if lang in in_set else -1]))\n",
    "        labels.to(device)\n",
    "\n",
    "        test.append((inputs,labels))\n",
    "        \n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "48261328",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0.4265449209196152\n",
      "0.3 0.45361160932659383\n",
      "0.4 0.48622207728680905\n",
      "0.5 0.5144301320723952\n",
      "0.6 0.4920919615196478\n",
      "0.7 0.4053481167454753\n",
      "0.8 0.28697211804989403\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(LOAD_PATH))\n",
    "\n",
    "for thresh in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "    correct = 0\n",
    "    for x, y in test:\n",
    "      # Send to GPU if available\n",
    "      if device is not None:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "      y_pred = net(x)\n",
    "      y_pred = torch.mean(y_pred, 1)\n",
    "      conf = y_pred.amax(1, keepdim=True)\n",
    "\n",
    "      if conf > thresh:\n",
    "          top_pred = y_pred.argmax(1, keepdim=True)\n",
    "      else:\n",
    "          top_pred = -1\n",
    "\n",
    "      if top_pred == y:\n",
    "        correct += 1\n",
    "        \n",
    "    print(thresh, correct / len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42500107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "ENG (In-set)\n",
      "GER (In-set)\n",
      "ICE (In-set)\n",
      "FRE (In-set)\n",
      "SPA (In-set)\n",
      "ARA (In-set)\n",
      "RUS (In-set)\n",
      "BEN (In-set)\n",
      "KAS (In-set)\n",
      "GRE (In-set)\n",
      "CAT (In-set)\n",
      "KOR (In-set)\n",
      "TUR (In-set)\n",
      "TAM (In-set)\n",
      "TEL (In-set)\n",
      "CHI (In-set)\n",
      "TIB (In-set)\n",
      "JAV (In-set)\n",
      "EWE (In-set)\n",
      "HAU (In-set)\n",
      "LIN (In-set)\n",
      "YOR (In-set)\n",
      "HUN (In-set)\n",
      "HAW (In-set)\n",
      "MAO (In-set)\n",
      "ITA (In-set)\n",
      "URD (In-set)\n",
      "SWE (In-set)\n",
      "PUS (In-set)\n",
      "GEO (In-set)\n",
      "HIN (In-set)\n",
      "THA (In-set)\n",
      "DUT (Out-of-set)\n",
      "HEB (Out-of-set)\n",
      "UKR (Out-of-set)\n",
      "BUL (Out-of-set)\n",
      "PER (Out-of-set)\n",
      "ALB (Out-of-set)\n",
      "UIG (Out-of-set)\n",
      "MAL (Out-of-set)\n",
      "BUR (Out-of-set)\n",
      "IBA (Out-of-set)\n",
      "ASA (Out-of-set)\n",
      "AKU (Out-of-set)\n",
      "ARM (Out-of-set)\n",
      "HRV (Out-of-set)\n",
      "FIN (Out-of-set)\n",
      "JPN (Out-of-set)\n",
      "NOR (Out-of-set)\n",
      "NEP (Out-of-set)\n",
      "RUM (Out-of-set)\n",
      "\n",
      "combining all mfccs by label\n",
      "chunking and splitting into 3 groups:\n",
      "\ttrain1: 246728\n",
      "\ttrain2: 86336\n",
      "\ttest: 34586\n",
      "\t\ttest (in-set): 13001\n",
      "\t\ttest (oos): 21585\n",
      "\n",
      "labels in test: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, -1}\n",
      "torch.Size([1, 400, 16])\n",
      "[(-1, 21585), (0, 213), (1, 449), (2, 449), (3, 449), (4, 450), (5, 450), (6, 449), (7, 448), (8, 152), (9, 177), (10, 423), (11, 449), (12, 450), (13, 318), (14, 256), (15, 448), (16, 450), (17, 449), (18, 450), (19, 449), (20, 450), (21, 450), (22, 450), (23, 449), (24, 449), (25, 450), (26, 450), (27, 450), (28, 450), (29, 450), (30, 226), (31, 449)]\n",
      "\n",
      "[(tensor([[[ 8.8736e+01,  4.2250e+00, -2.3506e+01,  ..., -5.8272e-01,\n",
      "           1.3767e-01, -1.7958e-01],\n",
      "         [ 8.8736e+01,  2.3721e+00, -2.4262e+01,  ..., -5.7503e-01,\n",
      "           1.0050e-01, -1.8968e-01],\n",
      "         [ 8.8554e+01,  7.0596e+00, -2.5397e+01,  ..., -5.3660e-01,\n",
      "           7.0755e-02, -1.7958e-01],\n",
      "         ...,\n",
      "         [ 1.0072e+02, -6.3710e+00, -3.7674e+00,  ..., -8.3389e-01,\n",
      "          -1.0736e-01, -4.6395e-02],\n",
      "         [ 1.0027e+02, -5.7459e+00, -3.3083e+00,  ..., -8.3389e-01,\n",
      "          -1.0736e-01, -6.7897e-02],\n",
      "         [ 1.0061e+02, -5.8501e+00, -3.9204e+00,  ..., -9.0759e-01,\n",
      "          -1.1276e-01, -4.0661e-02]]], device='cuda:0'), tensor([-1], device='cuda:0')), (tensor([[[ 1.0005e+02, -3.2456e+00, -5.4507e+00,  ..., -8.3777e-01,\n",
      "          -1.3165e-01, -6.9330e-02],\n",
      "         [ 9.8696e+01,  1.4426e+00, -7.8991e+00,  ..., -7.6019e-01,\n",
      "          -1.5863e-01, -1.0563e-01],\n",
      "         [ 9.6897e+01,  2.6928e+00, -6.5219e+00,  ..., -6.8262e-01,\n",
      "          -1.8561e-01, -3.3922e-03],\n",
      "         ...,\n",
      "         [ 9.3270e+01,  9.1985e+00, -1.0810e+01,  ..., -1.1614e+00,\n",
      "           2.4552e-01,  8.6914e-02],\n",
      "         [ 9.3203e+01,  9.7186e+00, -1.2958e+01,  ..., -1.0693e+00,\n",
      "           2.4552e-01, -1.2376e-01],\n",
      "         [ 9.2938e+01,  1.1799e+01, -1.3878e+01,  ..., -9.1805e-01,\n",
      "           2.3412e-01, -7.3630e-02]]], device='cuda:0'), tensor([-1], device='cuda:0')), (tensor([[[ 9.0683e+01,  1.3879e+01, -1.2037e+01,  ..., -6.7486e-01,\n",
      "           2.1134e-01, -1.4860e-02],\n",
      "         [ 8.6804e+01,  7.6384e+00,  6.7033e-01,  ..., -5.6237e-01,\n",
      "           1.9995e-01,  7.8313e-02],\n",
      "         [ 8.9954e+01, -1.2813e+01,  2.2943e+01,  ..., -5.3134e-01,\n",
      "           2.5691e-01,  9.2782e-02],\n",
      "         ...,\n",
      "         [ 9.8359e+01, -2.2432e-01, -1.0810e+01,  ..., -9.5751e-01,\n",
      "          -4.2603e-02, -7.6497e-02],\n",
      "         [ 9.7909e+01,  4.0077e-01, -1.2344e+01,  ..., -8.9983e-01,\n",
      "          -4.5301e-02, -5.0695e-02],\n",
      "         [ 9.8471e+01, -3.2850e-01, -1.2037e+01,  ..., -8.9983e-01,\n",
      "          -5.8793e-02, -1.8721e-01]]], device='cuda:0'), tensor([-1], device='cuda:0')), (tensor([[[ 9.8583e+01, -1.5954e-02, -9.5831e+00,  ..., -7.7183e-01,\n",
      "          -8.3077e-02, -2.3254e-01],\n",
      "         [ 9.7347e+01,  1.6509e+00, -7.1340e+00,  ..., -5.3910e-01,\n",
      "          -1.1006e-01, -1.4189e-01],\n",
      "         [ 9.4132e+01,  3.0053e+00,  3.8839e+00,  ..., -4.4601e-01,\n",
      "          -1.3435e-01, -9.6565e-02],\n",
      "         ...,\n",
      "         [ 9.9258e+01, -5.4334e+00, -1.2958e+01,  ..., -7.9228e-02,\n",
      "           7.4681e-01, -6.7671e-01],\n",
      "         [ 9.9933e+01, -3.8707e+00, -1.7559e+01,  ..., -1.3949e-01,\n",
      "           4.8477e-01, -6.1326e-01],\n",
      "         [ 9.7796e+01,  1.1300e+00, -2.0933e+01,  ..., -4.5377e-01,\n",
      "           4.3920e-01, -3.8664e-01]]], device='cuda:0'), tensor([-1], device='cuda:0')), (tensor([[[ 9.5657e+01,  3.6304e+00, -1.8479e+01,  ..., -9.4435e-01,\n",
      "           4.1641e-01, -3.3494e-02],\n",
      "         [ 9.4928e+01,  7.3783e+00, -1.4185e+01,  ..., -9.3778e-01,\n",
      "           4.1641e-01, -3.9228e-02],\n",
      "         [ 9.4795e+01,  8.1584e+00, -1.5105e+01,  ..., -9.9040e-01,\n",
      "           3.9363e-01, -1.5095e-01],\n",
      "         ...,\n",
      "         [ 7.0034e+01, -3.3701e+01, -1.3745e+00,  ..., -5.3215e-02,\n",
      "           2.9458e-01, -1.0685e-01],\n",
      "         [ 6.7696e+01, -3.2926e+01,  6.8793e+00,  ..., -3.5849e-01,\n",
      "           2.6535e-01, -2.5008e-01],\n",
      "         [ 7.8800e+01, -2.1311e+01, -3.2481e+00,  ..., -3.8624e-01,\n",
      "           1.9979e-01, -3.2170e-01]]], device='cuda:0'), tensor([-1], device='cuda:0'))]\n",
      "Loading the model\n",
      "Finished loading\n",
      "\n",
      "13001 \n",
      "\n",
      "Testing...\n",
      "iter: 2000 \tacc: 0.9275\n",
      "iter: 4000 \tacc: 0.93075\n",
      "iter: 6000 \tacc: 0.931\n",
      "iter: 8000 \tacc: 0.93075\n",
      "iter: 10000 \tacc: 0.9331\n",
      "iter: 12000 \tacc: 0.9325\n",
      "\n",
      "Acc: 0.9317744788862395\n"
     ]
    }
   ],
   "source": [
    "import kaldiio\n",
    "import sys, os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pickle import Unpickler\n",
    "from tdnn import TDNN\n",
    "from collections import defaultdict\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "in_set = ['ENG', 'GER', 'ICE', 'FRE', 'SPA', 'ARA', 'RUS', 'BEN', 'KAS', 'GRE', 'CAT', 'KOR', 'TUR', 'TAM', 'TEL', 'CHI', 'TIB', 'JAV', 'EWE', 'HAU', 'LIN', 'YOR', 'HUN', 'HAW', 'MAO', 'ITA', 'URD', 'SWE', 'PUS', 'GEO', 'HIN', 'THA']\n",
    "out_of_set = ['DUT', 'HEB', 'UKR', 'BUL', 'PER', 'ALB', 'UIG', 'MAL', 'BUR', 'IBA', 'ASA', 'AKU', 'ARM', 'HRV', 'FIN', 'JPN', 'NOR', 'NEP', 'RUM']\n",
    "\n",
    "root_dir = \"db/cu-multilang-dataset/\"\n",
    "\n",
    "\n",
    "data = []\n",
    "for i,lang in enumerate(in_set + out_of_set, 0):\n",
    "    print(lang, \"(In-set)\" if lang in in_set else \"(Out-of-set)\")\n",
    "    \n",
    "    for f_idx in range(1, 30 + 1):\n",
    "        \n",
    "        filepath = root_dir + lang + '/data/raw_mfcc_pitch_' + lang + '.' + str(f_idx) + '.ark'\n",
    "\n",
    "        for key, numpy_array in kaldiio.load_ark(filepath):\n",
    "            inputs = torch.from_numpy(np.expand_dims(numpy_array, axis=0))\n",
    "            inputs.to(device)\n",
    "            labels = torch.from_numpy(np.array([i if lang in in_set else -1]))\n",
    "            labels.to(device)\n",
    "            data.append((inputs, labels))\n",
    "\n",
    "\n",
    "print(\"\\ncombining all mfccs by label\")\n",
    "random.shuffle(data)\n",
    "data_concatenated = dict()\n",
    "for iter,i in enumerate(data): \n",
    "    label = i[1].numpy()[0]\n",
    "    mfcc = np.squeeze(i[0].numpy(), axis=0)\n",
    "    if label in data_concatenated:\n",
    "        data_concatenated[label].append(mfcc)\n",
    "    else:\n",
    "        data_concatenated[label] = [mfcc]\n",
    "\n",
    "for i in data_concatenated:\n",
    "    data_concatenated[i] = np.vstack(data_concatenated[i])\n",
    "\n",
    "del data\n",
    "\n",
    "def chunkify_tensor(tensor, size=400):\n",
    "    return torch.split(tensor, size, dim=1)[:-1] # except last one bc that isn't the right size\n",
    "\n",
    "'''\n",
    "Train (tdnn):       95% of in-set\n",
    "Train (lda/plda):   80% of out-of-set\n",
    "Test:               5% of in-set + 20% out-of-set\n",
    "'''\n",
    "\n",
    "print(\"chunking and splitting into 3 groups:\")\n",
    "train1, train2, test = [], [], []\n",
    "for i in data_concatenated:\n",
    "    label = torch.from_numpy(np.array([i]))\n",
    "    mfcc = torch.from_numpy(np.expand_dims(data_concatenated[i], axis=0))\n",
    "    chunks = chunkify_tensor(mfcc)\n",
    "    if i >= 0:\n",
    "        cutoff = int(len(chunks) * 0.95)\n",
    "        for chunk in chunks[:cutoff]:\n",
    "            train1.append((chunk.to(device), label.to(device)))\n",
    "        for chunk in chunks[cutoff:]:\n",
    "            test.append((chunk.to(device), label.to(device)))\n",
    "    else:\n",
    "        cutoff = int(len(chunks) * 0.8)\n",
    "        for chunk in chunks[:cutoff]:\n",
    "            train2.append((chunk.to(device), label.to(device)))\n",
    "        for chunk in chunks[cutoff:]:\n",
    "            test.append((chunk.to(device), label.to(device)))\n",
    "\n",
    "del data_concatenated\n",
    "\n",
    "print(\"\\ttrain1:\", len(train1))\n",
    "print(\"\\ttrain2:\", len(train2))\n",
    "print(\"\\ttest:\", len(test))\n",
    "print(\"\\t\\ttest (in-set):\", len([i for i in test if i[1].to('cpu').numpy()[0] >= 0]))\n",
    "print(\"\\t\\ttest (oos):\", len([i for i in test if i[1].to('cpu').numpy()[0] < 0]))\n",
    "print()\n",
    "print(\"labels in test:\", set([i[1].to('cpu').numpy()[0] for i in test]))\n",
    "print(test[0][0].size())\n",
    "asdfasdfasdf = defaultdict(int)\n",
    "for i in test:\n",
    "    asdfasdfasdf[i[1].to('cpu').numpy()[0]] += 1\n",
    "print(sorted(asdfasdfasdf.items()))\n",
    "print()\n",
    "\n",
    "\n",
    "print(test[:5])\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = TDNN(input_dim=in_size, output_dim=256, context_size=3)\n",
    "        self.layer2 = TDNN(input_dim=256, output_dim=256, context_size=3, dilation=1)\n",
    "        self.layer3 = TDNN(input_dim=256, output_dim=256, context_size=3, dilation=1)\n",
    "        self.layer4 = TDNN(input_dim=256, output_dim=256, context_size=1)\n",
    "        self.layer5 = TDNN(input_dim=256, output_dim=256, context_size=1)\n",
    "        self.final_layer = TDNN(input_dim=256, output_dim=num_classes, context_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        forward_pass = nn.Sequential(\n",
    "            self.layer1,\n",
    "            nn.ReLU(),\n",
    "            self.layer2,\n",
    "            nn.ReLU(),\n",
    "            self.layer3,\n",
    "            nn.ReLU(),\n",
    "            self.layer4,\n",
    "            nn.ReLU(),\n",
    "            self.layer5,\n",
    "            nn.ReLU(),\n",
    "            self.final_layer)\n",
    "        \n",
    "        return forward_pass(x)\n",
    "\n",
    "\n",
    "\n",
    "# LOAD_PATH = 'saved-models/tdnn-256-4s-25epochs'\n",
    "print('Loading the model')\n",
    "\n",
    "if len(sys.argv) != 2:\n",
    "    print(\"Usage: python3 decode.py <model_name> \\n Please refer to the saved_models directory to see available models and include the name without any extension.\")\n",
    "    exit(1)\n",
    "\n",
    "model_name = sys.argv[1]\n",
    "    \n",
    "infile = open(\"saved_models/\" + model_name + \".pickle\", \"rb\")\n",
    "net = Unpickler(infile).load()\n",
    "infile.close()\n",
    "\n",
    "print('Finished loading')\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "#only test on the in-set ones\n",
    "test = [i for i in test if i[1].to('cpu').numpy()[0] >= 0]\n",
    "random.shuffle(test)\n",
    "print(len(test), '\\n')\n",
    "\n",
    "\n",
    "\n",
    "print(\"Testing...\")\n",
    "correct, total = 0, 0\n",
    "\n",
    "for i, data in enumerate(test,0):\n",
    "    if i%2000 == 0 and total > 0:\n",
    "        print(\"iter:\", i, \"\\tacc:\", correct/total)\n",
    "\n",
    "    inputs, labels = data[0].to(device), data[1].to(device)\n",
    "    \n",
    "    outputs = net(inputs)\n",
    "    outputs = torch.mean(outputs, 1)\n",
    "    outputs = F.softmax(outputs, dim=1)\n",
    "    predicted = outputs.argmax(1, keepdim=True)\n",
    "\n",
    "    total += 1\n",
    "    if predicted == labels:\n",
    "        correct += 1\n",
    "\n",
    "print(\"\\nAcc:\", correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5880d1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
