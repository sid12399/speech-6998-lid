{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'torch.version' from '/home/sp4013/anaconda3/lib/python3.9/site-packages/torch/version.py'>\n",
      "cuda:0\n",
      "\n",
      "-----Getting Train/Test Data-----\n",
      "\n",
      "ENG (In-set)\n",
      "GER (In-set)\n",
      "ICE (In-set)\n",
      "FRE (In-set)\n",
      "SPA (In-set)\n",
      "ARA (In-set)\n",
      "RUS (In-set)\n",
      "BEN (In-set)\n",
      "KAS (In-set)\n",
      "GRE (In-set)\n",
      "CAT (In-set)\n",
      "KOR (In-set)\n",
      "TUR (In-set)\n",
      "TAM (In-set)\n",
      "TEL (In-set)\n",
      "CHI (In-set)\n",
      "TIB (In-set)\n",
      "JAV (In-set)\n",
      "EWE (In-set)\n",
      "HAU (In-set)\n",
      "LIN (In-set)\n",
      "YOR (In-set)\n",
      "HUN (In-set)\n",
      "HAW (In-set)\n",
      "MAO (In-set)\n",
      "ITA (In-set)\n",
      "URD (In-set)\n",
      "SWE (In-set)\n",
      "PUS (In-set)\n",
      "GEO (In-set)\n",
      "HIN (In-set)\n",
      "THA (In-set)\n",
      "\n",
      "Num chunks in training: 3944\n",
      "Num chunks in testing: 455\n",
      "\n",
      "-----Finished Data Splitting-----\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import kaldiio\n",
    "import torch\n",
    "\n",
    "\n",
    "random.seed(1)\n",
    "\n",
    "print(torch.version)\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "in_set = ['ENG', 'GER', 'ICE', 'FRE', 'SPA', 'ARA', 'RUS', 'BEN', 'KAS', 'GRE', 'CAT', 'KOR', 'TUR', 'TAM', 'TEL', 'CHI', 'TIB', 'JAV', 'EWE', 'HAU', 'LIN', 'YOR', 'HUN', 'HAW', 'MAO', 'ITA', 'URD', 'SWE', 'PUS', 'GEO', 'HIN', 'THA']\n",
    "out_of_set = ['DUT', 'HEB', 'UKR', 'BUL', 'PER', 'ALB', 'UIG', 'MAL', 'BUR', 'IBA', 'ASA', 'AKU', 'ARM', 'HRV', 'FIN', 'JPN', 'NOR', 'NEP', 'RUM']\n",
    "\n",
    "langs = in_set + out_of_set\n",
    "\n",
    "num_in_set = 32\n",
    "\n",
    "in_set = langs[:num_in_set]\n",
    "out_of_set = langs[num_in_set:]\n",
    "\n",
    "root_dir = \"/home/sp4013/kaldi-trunk/egs/lid/s1/db/cu-multilang-dataset/\"\n",
    "\n",
    "\n",
    "assert(len(in_set) + len(out_of_set) == 51)\n",
    "assert(len(set(in_set).intersection(set(out_of_set))) == 0)\n",
    "for lang in os.listdir(root_dir):\n",
    "    assert(lang in in_set or lang in out_of_set)\n",
    "\n",
    "print(\"\\n-----Getting Train/Test Data-----\\n\")\n",
    "\n",
    "train, test = [], []\n",
    "max_sample_length = 50000\n",
    "\n",
    "for i,lang in enumerate(in_set, 0):\n",
    "    print(lang, \"(In-set)\" if lang in in_set else \"(Out-of-set)\")\n",
    "    filepath = root_dir + lang + '/data/raw_mfcc_pitch_' + lang + '.1.ark'\n",
    "\n",
    "    chunks = []\n",
    "    for key, numpy_array in kaldiio.load_ark(filepath):\n",
    "        chunks +=  np.split(numpy_array, np.arange(max_sample_length, len(numpy_array), max_sample_length))\n",
    "    random.shuffle(chunks)\n",
    "\n",
    "    # Put chunks in train or test depending on in_set or out_of_set\n",
    "    switch_point = 0.90 if lang in in_set else 0.0\n",
    "    for j in range(len(chunks)):\n",
    "        chunk = chunks[j]\n",
    "        inputs = torch.from_numpy(np.expand_dims(chunk, axis=0))\n",
    "        inputs.to(device)\n",
    "        labels = torch.from_numpy(np.array([i if lang in in_set else -1]))\n",
    "        labels.to(device)\n",
    "\n",
    "        if j+1 <= switch_point * len(chunks):\n",
    "            train.append((inputs,labels))\n",
    "        else:\n",
    "            test.append((inputs,labels))\n",
    "\n",
    "print()\n",
    "print(\"Num chunks in training:\", len(train))\n",
    "print(\"Num chunks in testing:\", len(test))\n",
    "\n",
    "print(\"\\n-----Finished Data Splitting-----\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------\n",
      "      Layer (type)        Output Shape         Param #     Tr. Param #\n",
      "=======================================================================\n",
      "            TDNN-1       [1, 254, 256]          21,248          21,248\n",
      "            TDNN-2       [1, 250, 256]         197,376         197,376\n",
      "            TDNN-3       [1, 244, 256]         197,376         197,376\n",
      "            TDNN-4       [1, 244, 256]          66,304          66,304\n",
      "            TDNN-5       [1, 244, 256]          66,304          66,304\n",
      "            TDNN-6        [1, 244, 32]           8,288           8,288\n",
      "=======================================================================\n",
      "Total params: 556,896\n",
      "Trainable params: 556,896\n",
      "Non-trainable params: 0\n",
      "-----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "from tdnn import TDNN\n",
    "from pytorch_model_summary import summary\n",
    "\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, in_size, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer1 = TDNN(input_dim=in_size, output_dim=256, context_size=5)\n",
    "        self.layer2 = TDNN(input_dim=256, output_dim=256, context_size=3, dilation=2)\n",
    "        self.layer3 = TDNN(input_dim=256, output_dim=256, context_size=3, dilation=3)\n",
    "        self.layer4 = TDNN(input_dim=256, output_dim=256, context_size=1)\n",
    "        self.layer5 = TDNN(input_dim=256, output_dim=256, context_size=1)\n",
    "        self.final_layer = TDNN(input_dim=256, output_dim=num_classes, context_size=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        forward_pass = nn.Sequential(\n",
    "            self.layer1,\n",
    "            nn.ReLU(),\n",
    "            self.layer2,\n",
    "            nn.ReLU(),\n",
    "            self.layer3,\n",
    "            nn.ReLU(),\n",
    "            self.layer4,\n",
    "            nn.ReLU(),\n",
    "            self.layer5,\n",
    "            nn.ReLU(),\n",
    "            self.final_layer,\n",
    "            nn.Softmax(dim=2))\n",
    "        \n",
    "        return forward_pass(x)\n",
    "    \n",
    "print(summary(Net(16, len(in_set)), torch.zeros((1, 258, 16)), show_input=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# net = Net(16, len(in_set))\n",
    "# net.to(device)\n",
    "# criterion = nn.CrossEntropyLoss() # a common loss function for multi-class classification problems\n",
    "# optimizer = optim.SGD(net.parameters(), lr=0.001) # a common optimizer for multi-class classification problems\n",
    "\n",
    "# # Train the TDNN network\n",
    "# do_training = True # whether to train or to load a saved model\n",
    "# SAVE_PATH = 'saved_models/tdnn.pth'\n",
    "# LOAD_PATH = 'saved_models/tdnn.pth'\n",
    "\n",
    "# if do_training:\n",
    "#     print('Started Training')\n",
    "\n",
    "#     for epoch in range(12):  # number of epochs\n",
    "#         random.shuffle(train) # shuffle data every epoch\n",
    "#         print('\\tepoch: ', str(epoch + 1))\n",
    "#         running_loss = 0.0\n",
    "#         for i, data in enumerate(train, 0):\n",
    "#             inputs, labels = data[0].to(device), data[1].to(device)\n",
    "#             optimizer.zero_grad()\n",
    "\n",
    "#             outputs = net(inputs) # pass inputs thorugh the model\n",
    "# #             print(outputs[0])\n",
    "#             outputs = torch.mean(outputs, 1) #average over all time slices\n",
    "#             loss = criterion(outputs, labels) # compute loss\n",
    "#             loss.backward() # compute gradients\n",
    "\n",
    "# #             for name, param in net.named_parameters():\n",
    "# #                 print(name, param.grad)\n",
    "            \n",
    "#             optimizer.step() # SGD step\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "            \n",
    "#         print(\"epoch \" + str(epoch + 1) + \": \" + str(running_loss / len(train)))\n",
    "\n",
    "#     print('Finished Training')\n",
    "#     torch.save(net.state_dict(), SAVE_PATH) # Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, batch_size=64):\n",
    "    len_data = len(data)\n",
    "    num_batches = (len_data // batch_size) + 1\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min(start_idx + batch_size, len_data - 1)\n",
    "\n",
    "        yield data[start_idx: end_idx]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_pred, y):\n",
    "    top_pred = y_pred.argmax(1, keepdim=True)\n",
    "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
    "    acc = correct.float() / y.shape[0]\n",
    "    return acc\n",
    "\n",
    "\n",
    "def train_model(model, loader, optimizer, criterion, device=None):\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in loader:\n",
    "        num_batches += 1\n",
    "        len_batch = len(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss = 0.0\n",
    "        acc = 0.0\n",
    "        for x, y in batch:\n",
    "          # Send to GPU if available\n",
    "          if device is not None:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "          y_pred = model(x)\n",
    "          y_pred = torch.mean(y_pred, 1)\n",
    "          loss += criterion(y_pred, y)\n",
    "          acc += calculate_accuracy(y_pred, y)\n",
    "        \n",
    "        loss /= len_batch\n",
    "        acc /= len_batch\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        train_acc += acc.item()\n",
    "\n",
    "    return train_loss / num_batches, train_acc / num_batches\n",
    "\n",
    "\n",
    "def evaluate_model(model, loader, criterion, device=None):\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "#     model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            num_batches += 1\n",
    "            len_batch = len(batch)\n",
    "            loss = 0.0\n",
    "            acc = 0.0\n",
    "\n",
    "            for x, y in batch:\n",
    "              # Send to GPU if available\n",
    "              if device is not None:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device)\n",
    "\n",
    "              y_pred = model(x)\n",
    "              y_pred = torch.mean(y_pred, 1)\n",
    "              loss += criterion(y_pred, y)\n",
    "              acc += calculate_accuracy(y_pred, y)\n",
    "\n",
    "            loss /= len_batch\n",
    "            acc /= len_batch\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            val_acc += acc.item()\n",
    "\n",
    "    return val_loss / num_batches, val_acc / num_batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started Training\n",
      "Epoch: 0, Train Loss: 3.456104959211042, Train Accuracy: 0.15174971208457025, Val Loss: 3.4489848454793295, Val Accuracy: 0.27361111144224803\n",
      "Epoch: 1, Train Loss: 3.4411278463179067, Train Accuracy: 0.3833525346652154, Val Loss: 3.4339126427968343, Val Accuracy: 0.46180555621782937\n",
      "Epoch: 2, Train Loss: 3.422430461452853, Train Accuracy: 0.5285498273949469, Val Loss: 3.4117401123046873, Val Accuracy: 0.5138888895511627\n",
      "Epoch: 3, Train Loss: 3.397789091833176, Train Accuracy: 0.5915178572458606, Val Loss: 3.38860551516215, Val Accuracy: 0.5791666666666667\n",
      "Epoch: 4, Train Loss: 3.366714769794095, Train Accuracy: 0.671875, Val Loss: 3.3562095959981284, Val Accuracy: 0.6951388915379842\n",
      "Epoch: 5, Train Loss: 3.333521981393137, Train Accuracy: 0.742655530091255, Val Loss: 3.3257782141367596, Val Accuracy: 0.7270833333333333\n",
      "Epoch: 6, Train Loss: 3.3045206896720396, Train Accuracy: 0.7768217171392133, Val Loss: 3.305525239308675, Val Accuracy: 0.7354166666666667\n",
      "Epoch: 7, Train Loss: 3.276129263062631, Train Accuracy: 0.8206725235908262, Val Loss: 3.267752456665039, Val Accuracy: 0.8208333333333333\n",
      "Epoch: 8, Train Loss: 3.248097183242921, Train Accuracy: 0.8437139978331905, Val Loss: 3.259330638249715, Val Accuracy: 0.80625\n",
      "Epoch: 9, Train Loss: 3.228780907969321, Train Accuracy: 0.8645233300424391, Val Loss: 3.2245769023895265, Val Accuracy: 0.85625\n",
      "Epoch: 10, Train Loss: 3.2065735209372734, Train Accuracy: 0.8747119817041582, Val Loss: 3.2288729826609295, Val Accuracy: 0.81875\n",
      "Epoch: 11, Train Loss: 3.1916338493747096, Train Accuracy: 0.8797883064516129, Val Loss: 3.1939246972401936, Val Accuracy: 0.8729166666666667\n",
      "Epoch: 12, Train Loss: 3.1633453330686017, Train Accuracy: 0.8955213139134068, Val Loss: 3.177925221125285, Val Accuracy: 0.8791666666666667\n",
      "Epoch: 13, Train Loss: 3.150839744075652, Train Accuracy: 0.9014616935483871, Val Loss: 3.1558009306589763, Val Accuracy: 0.875\n",
      "Epoch: 14, Train Loss: 3.129166372360722, Train Accuracy: 0.915178571977923, Val Loss: 3.143511438369751, Val Accuracy: 0.8770833333333333\n",
      "Epoch: 15, Train Loss: 3.1191421478025374, Train Accuracy: 0.9153225806451613, Val Loss: 3.1383699576059976, Val Accuracy: 0.8708333333333333\n",
      "Epoch: 16, Train Loss: 3.1006530927073572, Train Accuracy: 0.9179507493972778, Val Loss: 3.1191035906473794, Val Accuracy: 0.875\n",
      "Epoch: 17, Train Loss: 3.091752654121768, Train Accuracy: 0.9203269010589968, Val Loss: 3.1015295664469402, Val Accuracy: 0.9041666666666667\n",
      "Epoch: 18, Train Loss: 3.0734398249656922, Train Accuracy: 0.9294354838709677, Val Loss: 3.0948054790496826, Val Accuracy: 0.8854166666666666\n",
      "Epoch: 19, Train Loss: 3.0583371769997383, Train Accuracy: 0.9338277655263101, Val Loss: 3.0851346015930177, Val Accuracy: 0.8916666666666667\n",
      "Epoch: 20, Train Loss: 3.0517451513198113, Train Accuracy: 0.9329637096774194, Val Loss: 3.087041695912679, Val Accuracy: 0.8583333333333333\n",
      "Epoch: 21, Train Loss: 3.0462012483227636, Train Accuracy: 0.9279233870967742, Val Loss: 3.066987180709839, Val Accuracy: 0.88125\n",
      "Epoch: 22, Train Loss: 3.0302132906452304, Train Accuracy: 0.9338277655263101, Val Loss: 3.055859851837158, Val Accuracy: 0.8833333333333333\n",
      "Epoch: 23, Train Loss: 3.0168377295617135, Train Accuracy: 0.9439084106876005, Val Loss: 3.04358065923055, Val Accuracy: 0.8958333333333334\n",
      "Epoch: 24, Train Loss: 3.011201200946685, Train Accuracy: 0.9448084677419355, Val Loss: 3.035078191757202, Val Accuracy: 0.8958333333333334\n",
      "Best Val Loss: 3.035078191757202 at Epoch: 24\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TDNN, loss, and optimizer\n",
    "# print(len(in_set))\n",
    "net = Net(16, len(in_set))\n",
    "net.to(device)\n",
    "criterion = nn.CrossEntropyLoss() # a common loss function for multi-class classification problems\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001) # a common optimizer for multi-class classification problems\n",
    "\n",
    "\n",
    "# Train the TDNN network\n",
    "do_training = True # whether to train or to load a saved model\n",
    "SAVE_PATH = 'saved_models/tdnn.pth'\n",
    "LOAD_PATH = 'saved_models/tdnn.pth'\n",
    "\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "\n",
    "if do_training:\n",
    "    print('Started Training')\n",
    "\n",
    "    for epoch in range(25):  # number of epochs\n",
    "        random.shuffle(train) # shuffle data every epoch\n",
    "        train_loss, train_accuracy = train_model(net, create_batches(train, 32), optimizer, criterion, device)\n",
    "        val_loss, val_accuracy = evaluate_model(net, create_batches(test, 32), criterion, device)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        if val_loss < best_loss:\n",
    "          best_loss = val_loss\n",
    "          best_epoch = epoch\n",
    "          torch.save(net.state_dict(), SAVE_PATH) # Save the model\n",
    "\n",
    "\n",
    "        print(\"Epoch: \" + str(epoch) + \", Train Loss: \" + str(train_loss) + \", Train Accuracy: \" + str(train_accuracy) + \\\n",
    "            \", Val Loss: \" + str(val_loss) + \", Val Accuracy: \" + str(val_accuracy))\n",
    "\n",
    "\n",
    "    print(\"Best Val Loss: \" + str(best_loss) + \" at Epoch: \" + str(best_epoch))\n",
    "                \n",
    "    print('Finished Training')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DUT (Out-of-set)\n",
      "HEB (Out-of-set)\n",
      "UKR (Out-of-set)\n",
      "BUL (Out-of-set)\n",
      "PER (Out-of-set)\n",
      "ALB (Out-of-set)\n",
      "UIG (Out-of-set)\n",
      "MAL (Out-of-set)\n",
      "BUR (Out-of-set)\n",
      "IBA (Out-of-set)\n",
      "ASA (Out-of-set)\n",
      "AKU (Out-of-set)\n",
      "ARM (Out-of-set)\n",
      "HRV (Out-of-set)\n",
      "FIN (Out-of-set)\n",
      "JPN (Out-of-set)\n",
      "NOR (Out-of-set)\n",
      "NEP (Out-of-set)\n",
      "RUM (Out-of-set)\n",
      "2231\n"
     ]
    }
   ],
   "source": [
    "# Add out of set to test set\n",
    "\n",
    "for i,lang in enumerate(out_of_set, 0):\n",
    "    print(lang, \"(In-set)\" if lang in in_set else \"(Out-of-set)\")\n",
    "    filepath = root_dir + lang + '/data/raw_mfcc_pitch_' + lang + '.1.ark'\n",
    "\n",
    "    chunks = []\n",
    "    for key, numpy_array in kaldiio.load_ark(filepath):\n",
    "        chunks +=  np.split(numpy_array, np.arange(max_sample_length, len(numpy_array), max_sample_length))\n",
    "    random.shuffle(chunks)\n",
    "\n",
    "    for j in range(len(chunks)):\n",
    "        chunk = chunks[j]\n",
    "        inputs = torch.from_numpy(np.expand_dims(chunk, axis=0))\n",
    "        inputs.to(device)\n",
    "        labels = torch.from_numpy(np.array([i if lang in in_set else -1]))\n",
    "        labels.to(device)\n",
    "\n",
    "        test.append((inputs,labels))\n",
    "        \n",
    "print(len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2 0.31958762886597936\n"
     ]
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(LOAD_PATH))\n",
    "\n",
    "for thresh in [0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8]:\n",
    "    correct = 0\n",
    "    for x, y in test:\n",
    "      # Send to GPU if available\n",
    "      if device is not None:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "      y_pred = net(x)\n",
    "      y_pred = torch.mean(y_pred, 1)\n",
    "      conf = y_pred.amax(1, keepdim=True)\n",
    "\n",
    "      if conf > thresh:\n",
    "          top_pred = y_pred.argmax(1, keepdim=True)\n",
    "      else:\n",
    "          top_pred = -1\n",
    "\n",
    "      if top_pred == y:\n",
    "        correct += 1\n",
    "        \n",
    "    print(thresh, correct / len(test))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
